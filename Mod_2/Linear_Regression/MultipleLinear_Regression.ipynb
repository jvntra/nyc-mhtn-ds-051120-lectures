{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Off?\n",
    "\n",
    "You are a data scientist for the MTA. For your first project, they want you to predict the number of subway riders for each day. You decide to do a linear regression model predict the riders but need to gather data first. With a partner brainstorm a list of different variables you think would explain the number of daily riders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "Agenda Today:\n",
    "\n",
    "- Create a model for multiple linear regression\n",
    "- Interpret the output for multiple linear regression\n",
    "- Multicollinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple linear regression in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in car data\n",
    "df = sns.load_dataset('mpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear regression model using statsmodel \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "lr_model = ols(formula='mpg~weight', data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "Multiple linear regression is simply a linear regression with more than one predictor, or independent variables. Let's recall the interpretation of $R^2$ in simple linear regression represents the proportion of variance explained by the model. What if we make the model more complex by including more predictors in it such that it account for even more variance in the outcome?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + + \\beta_3 X_3\\cdots + \\beta_k X_k + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "sen = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((sen/10), 0.5)\n",
    "sen = sen.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "fig.suptitle('seniority vs. income', fontsize=16)\n",
    "plt.scatter(sen, income)\n",
    "plt.plot(sen, sen/10, c = \"black\")\n",
    "plt.xlabel(\"seniority\", fontsize=14)\n",
    "plt.ylabel(\"monthly income\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we include another factor, such as years of education? All that is doing is adding a higher dimensional object to the model, so our model will be three dimensional. \n",
    "<img src=\"multi_reg_graph.png\" style=\"withd:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are 3 major uses for multiple linear regression analysis.  \n",
    "\n",
    "1. Useful to identify the strength of the effect that the independent variables have on a dependent variable.\n",
    "\n",
    "2. Forecast effects or impacts of changes.  That is, multiple linear regression analysis helps us to understand how much will the dependent variable change when we change the independent variables.  For instance, a multiple linear regression can tell you how much GPA is expected to increase (or decrease) for every one point increase (or decrease) in IQ.\n",
    "\n",
    "3. Predicts trends and future values.  The multiple linear regression analysis can be used to get point estimates.  An example question may be “what will the price of gold be 6 month from now?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~weight+horsepower+displacement+cylinders+acceleration', data=df).fit()\n",
    "mlr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the Model Parameters\n",
    "- Each β parameter represents the change in the mean response, E(y), per unit increase in the associated predictor variable when all the other predictors are held constant.\n",
    "- For example, β1 represents the estimated change in the mean response, E(y), per unit increase in x1 when x2, x3, ..., xp−1 are held constant.\n",
    "- The intercept term, β0, represents the estimated mean response, E(y), when all the predictors x1, x2, ..., xp−1, are all zero (which may or may not have any practical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity \n",
    "\n",
    "**Multicollinearity** occurs when independent variables in a regression model are very highly correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic kinds of multicollinearity:\n",
    "\n",
    "- **Structural multicollinearity:** This type occurs when we create a model term using other terms. In other words, it’s a byproduct of the model that we specify rather than being present in the data itself. For example, if you square term X to model curvature, clearly there is a correlation between X and X2.\n",
    "- **Data multicollinearity:** This type of multicollinearity is present in the data itself rather than being an artifact of our model. Observational experiments are more likely to exhibit this kind of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Problems Do Multicollinearity Cause?\n",
    "\n",
    "Multicollinearity causes the following two basic types of problems:\n",
    "\n",
    "- The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model.\n",
    "- Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I Have to Fix Multicollinearity?\n",
    "\n",
    "The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
    "\n",
    "- The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "- Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. \n",
    "- Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***That being said, the easies way to deal with multicollinearity is just to remove one of the variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df.corr())\n",
    "plt.xticks(range(len(df.columns)), df.columns)\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Better Looking Heatmap with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrMtx(df, dropDuplicates = True):\n",
    "\n",
    "    # Your dataset is already a correlation matrix.\n",
    "    # If you have a dateset where you need to include the calculation\n",
    "    # of a correlation matrix, just uncomment the line below:\n",
    "    # df = df.corr()\n",
    "\n",
    "    # Exclude duplicate correlations by masking uper right values\n",
    "    if dropDuplicates:    \n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set background color / chart style\n",
    "    sns.set_style(style = 'white')\n",
    "\n",
    "    # Set up  matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Add diverging colormap from red to blue\n",
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "    # Draw correlation plot with or without duplicates\n",
    "    if dropDuplicates:\n",
    "        sns.heatmap(df, mask=mask, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "    else:\n",
    "        sns.heatmap(df, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "\n",
    "CorrMtx(corr, dropDuplicates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more examples to make your correlation heatmap look good\n",
    "https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun the Model After Removing the highly correlate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~____', data=____).fit()\n",
    "mlr_model.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['origin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the column of `origin`, we can see that the values come through as strings that represent a category.  We can not put a string through as a value for a linear model. Instead we use dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **dummy variable** (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.\n",
    "\n",
    "Technically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values. As a practical matter, regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['origin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating these dummy variables. We can now include them in the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = ols(formula='mpg~weight+horsepower+cylinders+acceleration+origin_europe+origin_japan', data=dummy_df).fit()\n",
    "dummy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that I did not include the `origin_usa` column in this model.\n",
    "\n",
    "That dummy variable would redundant; it carries no new information. And it creates a severe multicollinearity problem for the analysis. \n",
    "\n",
    "If we know the value of the `origin_europe` and the `origin'japan` columns, then we also know the value of the `origin_usa` column.\n",
    "\n",
    "Using all the dummy variables off a category, is known as the dummy variable trap. Avoid this trap!\n",
    "\n",
    "There are tow ways to handle this. One is as seen above where I just don't include one of the variables in the model. Another is to automatically drop one of the columns when I use `pd.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['origin'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients of Dummy Variables\n",
    "\n",
    "So how do we interpret the coefficient of a dummy variable? Looking at our model output from above. If a car originated in Japan, it will have a value of 1 for the variable `origin_japan`. So we would multiply the value of 1 by the coefficient for that variable and add that to our final MPG prediction.  \n",
    "\n",
    "If the car orginated in the USA it would have a value of zero for both the `origin_japan` and `origin_europe` columns. Therefore, the coefficients for those variables who not impact the final prediction.  \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On average what is the difference in the gas mileage for cars from Japan verus cars from USA in this dataset?**\n",
    "\n",
    "**On average what is the difference in the gas mileage for cars from Japan verus cars from Europe in this dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data\n",
    "\n",
    "\n",
    "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. For linear regression models, this makes it difficult to compare the sizes of the coefficients for different variables. Also, many machine learning algorithms use Eucledian distance between two data points in their computations, and data of different scale, distorts those distances.\n",
    "\n",
    "Three common ways to scale the data are:\n",
    "1. **Standardization**: This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python. \n",
    "$$x' =\\frac{x-\\bar{x}}{\\sigma}$$\n",
    "\n",
    "2. **Mean Normalization**: This distribution will have values between -1 and 1 with μ=0.\n",
    "\n",
    "$$x' =\\frac{x-\\bar{x}}{max(x)- min(x)}$$\n",
    "\n",
    "3. **Min-Max Scaling**: This scaling brings the value between 0 and 1.\n",
    "\n",
    "$$x' =\\frac{x-min(x)}{max(x)- min(x)}$$\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler.fit_transform(dummy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.drop(['name', 'origin_usa'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform(dummy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Another way to use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset= ['cylinders', 'horsepower', 'weight',\n",
    "       'acceleration', 'origin_europe', 'origin_japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_scaled = []\n",
    "for var in subset:\n",
    "    new_col = var +\"_scaled\"\n",
    "    dummy_df[new_col] = scaler.fit_transform(dummy_df[[var]])\n",
    "    subset_scaled.append(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_model = ols(formula='mpg~weight_scaled+horsepower_scaled+cylinders_scaled+acceleration_scaled+origin_europe_scaled+origin_japan_scaled', data=dummy_df).fit()\n",
    "scaled_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our coefficients are on a similiar scale which allows us to compare the size of the coeffiecents to make some inferences about which features have a bigger impact on the MPG of a car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Everything about regression:  https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-tutorial-and-examples\n",
    "\n",
    "Statsmodels example: https://datatofish.com/statsmodels-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
